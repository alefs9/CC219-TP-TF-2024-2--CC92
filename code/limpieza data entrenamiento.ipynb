{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3d22cf",
   "metadata": {},
   "source": [
    "# Parte 3: Limpieza del texto para modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8261277",
   "metadata": {},
   "source": [
    "El preprocesamiento del texto es esencial para reducir el ruido y mejorar la calidad del aprendizaje automático. El objetivo de esta etapa es:\n",
    "\n",
    "- Eliminar palabras genéricas o irrelevantes\n",
    "- Reducir vocabulario redundante (palabras con frecuencia 1)\n",
    "- Filtrar tokens anómalos (muy largos o poco informativos)\n",
    "- Optimizar el corpus para enfoques como TF-IDF y embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56c6dd",
   "metadata": {},
   "source": [
    "### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1dfc2c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tabulate in c:\\users\\alexa\\appdata\\roaming\\python\\python312\\site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "178a8cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d75d2b",
   "metadata": {},
   "source": [
    "### Carga y Limpieza del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5b59291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_23724\\1043033834.py:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  data_limpia = pd.read_csv('C:\\CC219-TP-TF-2024-2--CC92\\data\\data_limpia.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (8469, 11)\n",
      "\n",
      "Primeras filas del dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer Age</th>\n",
       "      <th>Customer Gender</th>\n",
       "      <th>Product Purchased</th>\n",
       "      <th>Date of Purchase</th>\n",
       "      <th>Ticket Type</th>\n",
       "      <th>Ticket Subject</th>\n",
       "      <th>Ticket Description</th>\n",
       "      <th>Ticket Status</th>\n",
       "      <th>Ticket Priority</th>\n",
       "      <th>Ticket Channel</th>\n",
       "      <th>Hours_to_First_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>Other</td>\n",
       "      <td>GoPro Hero</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>Product setup</td>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Pending Customer Response</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Social media</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>LG Smart TV</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>Peripheral compatibility</td>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Pending Customer Response</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>Other</td>\n",
       "      <td>Dell XPS</td>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>Network problem</td>\n",
       "      <td>I'm facing a problem with my {product_purchase...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social media</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>Female</td>\n",
       "      <td>Microsoft Office</td>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>Billing inquiry</td>\n",
       "      <td>Account access</td>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social media</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>Female</td>\n",
       "      <td>Autodesk AutoCAD</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>Billing inquiry</td>\n",
       "      <td>Data loss</td>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Low</td>\n",
       "      <td>Email</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer Age Customer Gender Product Purchased Date of Purchase  \\\n",
       "0            32           Other        GoPro Hero       2021-03-22   \n",
       "1            42          Female       LG Smart TV       2021-05-22   \n",
       "2            48           Other          Dell XPS       2020-07-14   \n",
       "3            27          Female  Microsoft Office       2020-11-13   \n",
       "4            67          Female  Autodesk AutoCAD       2020-02-04   \n",
       "\n",
       "       Ticket Type            Ticket Subject  \\\n",
       "0  Technical issue             Product setup   \n",
       "1  Technical issue  Peripheral compatibility   \n",
       "2  Technical issue           Network problem   \n",
       "3  Billing inquiry            Account access   \n",
       "4  Billing inquiry                 Data loss   \n",
       "\n",
       "                                  Ticket Description  \\\n",
       "0  I'm having an issue with the {product_purchase...   \n",
       "1  I'm having an issue with the {product_purchase...   \n",
       "2  I'm facing a problem with my {product_purchase...   \n",
       "3  I'm having an issue with the {product_purchase...   \n",
       "4  I'm having an issue with the {product_purchase...   \n",
       "\n",
       "               Ticket Status Ticket Priority Ticket Channel  \\\n",
       "0  Pending Customer Response        Critical   Social media   \n",
       "1  Pending Customer Response        Critical           Chat   \n",
       "2                     Closed             Low   Social media   \n",
       "3                     Closed             Low   Social media   \n",
       "4                     Closed             Low          Email   \n",
       "\n",
       "   Hours_to_First_Response  \n",
       "0                      7.0  \n",
       "1                      6.0  \n",
       "2                      7.0  \n",
       "3                      6.0  \n",
       "4                     20.0  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset limpio\n",
    "data_limpia = pd.read_csv('C:\\CC219-TP-TF-2024-2--CC92\\data\\data_limpia.csv')\n",
    "\n",
    "# Revisar el dataset\n",
    "print(\"Dimensiones del dataset:\", data_limpia.shape)\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "data_limpia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e5848512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables objetivo\n",
    "# Se consideran \"urgentes\" los tickets con prioridad \"Critical\" o \"High\" (valor 1), el resto son \"no urgentes\" (valor 0)\n",
    "data_limpia[\"Urgency\"] = data_limpia[\"Ticket Priority\"].apply(lambda x: 1 if x in [\"Critical\", \"High\"] else 0)\n",
    "\n",
    "# Tickets con tiempo de respuesta igual o menor a 1 hora toman el valor de 1, el resto como 0\n",
    "data_limpia[\"Resolution_Time_Bin\"] = data_limpia[\"Hours_to_First_Response\"].apply(lambda x: 1 if x <= 1 else 0)\n",
    "\n",
    "# Filtrar columnas relevantes\n",
    "data_limpia = data_limpia[[\"Ticket Description\", \"Ticket Priority\", \"Ticket Channel\", \"Urgency\", \"Resolution_Time_Bin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a2f1efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urgencia:\n",
      "- No urgente (0): 4255 (50.24%)\n",
      "- Urgente (1): 4214 (49.76%)\n",
      "\n",
      "Tiempo de resolución:\n",
      "- Más de una hora (0): 8140 (96.12%)\n",
      "- Menos de una hora (1): 329 (3.88%)\n"
     ]
    }
   ],
   "source": [
    "# Verificación\n",
    "total = len(data_limpia)\n",
    "\n",
    "# Urgencia\n",
    "urg_counts = data_limpia[\"Urgency\"].value_counts()\n",
    "print(f\"Urgencia:\")\n",
    "print(f\"- No urgente (0): {urg_counts[0]} ({urg_counts[0] / total:.2%})\")\n",
    "print(f\"- Urgente (1): {urg_counts[1]} ({urg_counts[1] / total:.2%})\\n\")\n",
    "\n",
    "# Tiempo de resolución\n",
    "res_counts = data_limpia[\"Resolution_Time_Bin\"].value_counts()\n",
    "print(f\"Tiempo de resolución:\")\n",
    "print(f\"- Más de una hora (0): {res_counts[0]} ({res_counts[0] / total:.2%})\")\n",
    "print(f\"- Menos de una hora (1): {res_counts[1]} ({res_counts[1] / total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "78db2c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket Description</th>\n",
       "      <th>Ticket Priority</th>\n",
       "      <th>Ticket Channel</th>\n",
       "      <th>Urgency</th>\n",
       "      <th>Resolution_Time_Bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Social media</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm facing a problem with my {product_purchase...</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social media</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social media</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Low</td>\n",
       "      <td>Email</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Ticket Description Ticket Priority  \\\n",
       "0  I'm having an issue with the {product_purchase...        Critical   \n",
       "1  I'm having an issue with the {product_purchase...        Critical   \n",
       "2  I'm facing a problem with my {product_purchase...             Low   \n",
       "3  I'm having an issue with the {product_purchase...             Low   \n",
       "4  I'm having an issue with the {product_purchase...             Low   \n",
       "\n",
       "  Ticket Channel  Urgency  Resolution_Time_Bin  \n",
       "0   Social media        1                    0  \n",
       "1           Chat        1                    0  \n",
       "2   Social media        0                    0  \n",
       "3   Social media        0                    0  \n",
       "4          Email        0                    0  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_limpia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93819ba",
   "metadata": {},
   "source": [
    "Se crean dos variables clave para entrenar los modelos de clasificación:\n",
    "- Urgency: marca los tickets como urgentes (1) si su prioridad es Critical o High, y como no urgentes (0) en otros casos.\n",
    "- Resolution_Time_Bin: indica si el ticket recibió una primera respuesta en menos de una hora (1) o no (0).\n",
    "\n",
    "Luego, se filtran las columnas más relevantes: la descripción del ticket, el canal por el que ingresó y las dos nuevas etiquetas objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5c820",
   "metadata": {},
   "source": [
    "Estas variables son relevantes porque se relacionan directamente con los insights obtenidos:\n",
    "\n",
    "* **Urgency** permite identificar si un ticket es crítico o no según su prioridad. Esto es útil para entrenar modelos que ayuden a mejorar la **priorización automática de tickets urgentes** (Insight 1).\n",
    "\n",
    "* **Resolution\\_Time\\_Bin** indica si la primera respuesta fue rápida (menos de una hora). Al relacionarla con el canal de entrada, permite evaluar el **desempeño de los canales** y encontrar oportunidades de mejora (Insight 2).\n",
    "\n",
    "* **Ticket Description** proporciona el texto del problema, que puede contener patrones comunes relacionados con urgencia o demoras, ayudando a **detectar temas frecuentes y optimizar la respuesta** (Insight 3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e7b77",
   "metadata": {},
   "source": [
    "### Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4c264a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Limpieza de texto\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\{.*?\\}', '', text)                    # Eliminar placeholders como {product_purchased}\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)                # Eliminar todo excepto letras y espacios\n",
    "    text = text.lower()                                    # Pasar todo a minúsculas\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)                # Eliminar palabras de 1 o 2 letras (como \"i\", \"as\", \"is\")\n",
    "    text = re.sub(r'\\s+', ' ', text)                       # Reemplazar múltiples espacios por uno solo\n",
    "    words = text.split()                                   # Divide el texto en palabras individuales\n",
    "    words = [w for w in words if w not in stop_words]      # Elimina stopwords\n",
    "    return ' '.join(words)                                 # Reconstruye el texto limpio como una sola cadena\n",
    "\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].apply(clean_text)\n",
    "\n",
    "# Codificación de variables categóricas\n",
    "categorical_features = [\"Ticket Priority\", \"Ticket Channel\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Pipeline para vectorizar el texto con TF-IDF (limita a las 5000 palabras más relevantes y elimina stopwords adicionales)\n",
    "text_transformer = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english'))])\n",
    "\n",
    "# Combinación de transformadores: texto (TF-IDF) + variables categóricas (one-hot)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', text_transformer, \"Ticket Description\"),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1050bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del conjunto transformado: (8469, 5008)\n",
      "ive encountered data loss issue files documents seem disappeared guide retrieve meantime ill continue develop new products thank ive checked available software updates none\n",
      "\n",
      "Categorías codificadas: ['Ticket Priority_Critical' 'Ticket Priority_High' 'Ticket Priority_Low']\n",
      "Palabras TF-IDF más comunes: ['ability' 'able' 'abovementioned' 'abovethen' 'abroad']\n"
     ]
    }
   ],
   "source": [
    "# Aplicar el preprocesamiento a una muestra\n",
    "X_preprocessed = preprocessor.fit_transform(data_limpia)\n",
    "\n",
    "# Imprimir forma del resultado\n",
    "print(\"Forma del conjunto transformado:\", X_preprocessed.shape)\n",
    "\n",
    "# Verificar limpieza de texto\n",
    "print(data_limpia[\"Ticket Description\"].sample(1).values[0])\n",
    "\n",
    "# Verificar categorías detectadas\n",
    "ohe = preprocessor.named_transformers_['cat']['onehot']\n",
    "print(\"\\nCategorías codificadas:\", ohe.get_feature_names_out(categorical_features)[:3])\n",
    "\n",
    "# Verificar palabras clave extraídas del texto\n",
    "tfidf = preprocessor.named_transformers_['tfidf']['tfidf']\n",
    "print(\"Palabras TF-IDF más comunes:\", tfidf.get_feature_names_out()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a6ce7",
   "metadata": {},
   "source": [
    "### Interpretación de la salida transformada\n",
    "\n",
    "- **Forma del conjunto transformado: (8469, 5008)**  \n",
    "  Esto indica que, después de aplicar la transformación de texto y la codificación de variables categóricas, contamos con un total de 8469 ejemplos (tickets) y 5008 características distintas por ejemplo. Estas características incluyen tanto las palabras relevantes del texto como las variables categóricas transformadas (como prioridad o canal del ticket).\n",
    "\n",
    "- **Ejemplo de texto limpio:**  \n",
    "  Se muestra un ejemplo del contenido de un ticket después de la limpieza. En esta etapa, se eliminaron caracteres especiales, palabras vacías (stopwords), y palabras poco informativas, dejando solo los términos relevantes para el modelo. Esto mejora la calidad de la representación del texto.\n",
    "\n",
    "- **Categorías codificadas:**  \n",
    "  Se listan las variables categóricas transformadas mediante codificación one-hot. En este caso, corresponden a los diferentes niveles de prioridad del ticket. Cada una se convierte en una columna binaria que indica su presencia.\n",
    "\n",
    "- **Palabras TF-IDF más comunes:**  \n",
    "  Aquí se muestran algunas de las palabras con mayor peso o frecuencia relativa dentro del corpus, después de aplicar la vectorización TF-IDF. Estas palabras representan los términos más relevantes para el conjunto de tickets según su frecuencia y distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "dfb8834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más comunes según TF-IDF: ['issue' 'assist' 'ive' 'problem' 'product' 'software' 'steps' 'data'\n",
      " 'persists' 'noticed' 'resolve' 'update' 'device' 'help' 'unable' 'tried'\n",
      " 'started' 'need' 'times' 'using']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = preprocessor.named_transformers_['tfidf'].named_steps['tfidf']\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf_scores = vectorizer.idf_\n",
    "\n",
    "# Mostrar top 20 palabras con menor IDF (más frecuentes)\n",
    "top_indices = np.argsort(idf_scores)[:20]\n",
    "top_words = feature_names[top_indices]\n",
    "\n",
    "print(\"Palabras más comunes según TF-IDF:\", top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2301e28",
   "metadata": {},
   "source": [
    "Imprimir las 20 palabras más comunes nos permite tener una visión general de qué términos están teniendo mayor peso dentro del corpus de texto. Esta revisión es útil para validar si el proceso de limpieza textual fue efectivo, así como para identificar si las palabras que destacan son realmente relevantes para el análisis. También puede ayudarnos a detectar ruido, redundancias o términos poco informativos que sería conveniente filtrar antes de entrenar un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a742f",
   "metadata": {},
   "source": [
    "Estas palabras reflejan un patrón: \n",
    "- Los clientes reportan problemas técnicos (bugs, errores en software/dispositivos).\n",
    "- Muchos buscan ayuda inmediata (“help”, “resolve”, “unable”, “need”).\n",
    "- Algunos describen acciones previas intentadas (“tried”, “steps”, “started”).\n",
    "\n",
    "Este hallazgo respalda el Insight 3 del EDA: la ausencia de un problema dominante refleja múltiples frentes de mejora operativa.\n",
    "\n",
    "La diversidad de palabras recurrentes sugiere que los clientes enfrentan una amplia gama de inconvenientes, no un único problema repetido. Por tanto, una solución automatizada debe aprender a priorizar casos basándose en estas palabras clave y su contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9f1c3d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1825    issue please assist thanks return thanks under...\n",
       "473     issue please assist protected stringproperty n...\n",
       "3509    encountering software bug whenever try perform...\n",
       "5415    issue please assist please help order fulfillm...\n",
       "5537    issue please assist gnome mmt addressing servi...\n",
       "Name: Ticket Description, dtype: object"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_limpia[\"Ticket Description\"].sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a7adfc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1825        return  understanding  sharing   forget   ...\n",
       "473        protected stringproperty name productpurcha...\n",
       "3509    encountering software bug whenever try perform...\n",
       "5415         order fulfillment process  patience patie...\n",
       "5537       gnome mmt addressing service found device s...\n",
       "Name: Ticket Description, dtype: object"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_phrases = ['assist', 'please', 'help', 'issue', 'thanks', 'thank', 'sorry', 'ive' ]\n",
    "def remove_generic_phrases(text):\n",
    "    for phrase in generic_phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "    return text\n",
    "\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].apply(remove_generic_phrases)\n",
    "data_limpia[\"Ticket Description\"].sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fdf565da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palabras con frecuencia 1: 3716\n",
      "Número de palabras con más de 25 caracteres: 161\n",
      "Número de palabras con caracteres no alfabéticos: 0\n"
     ]
    }
   ],
   "source": [
    "todas_las_palabras = ' '.join(data_limpia[\"Ticket Description\"]).split()\n",
    "\n",
    "# Contar frecuencia de cada palabra\n",
    "word_freq = Counter(todas_las_palabras)\n",
    "\n",
    "# 1. Palabras con frecuencia 1\n",
    "palabras_frec_1 = [word for word, freq in word_freq.items() if freq == 1]\n",
    "\n",
    "# 2. Palabras con más de 25 caracteres\n",
    "palabras_largas = [word for word in word_freq if len(word) > 25]\n",
    "\n",
    "# 3. Palabras que contienen algo no alfabético\n",
    "palabras_no_alfabeticas = [word for word in word_freq if not word.isalpha()]\n",
    "\n",
    "# Imprimir resumen\n",
    "print(f\"Número de palabras con frecuencia 1: {len(palabras_frec_1)}\")\n",
    "print(f\"Número de palabras con más de 25 caracteres: {len(palabras_largas)}\")\n",
    "print(f\"Número de palabras con caracteres no alfabéticos: {len(palabras_no_alfabeticas)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e79cd",
   "metadata": {},
   "source": [
    "Aunque se realizó una limpieza básica del texto (eliminando caracteres no alfabéticos, palabras cortas y stopwords), aún persisten ciertos indicadores de \"ruido\" residual en el corpus. Por ejemplo, se detectaron 3720 palabras con frecuencia 1, lo que puede incluir errores tipográficos, nombres propios poco comunes o tokens irrelevantes. Además, se identificaron 162 palabras con más de 25 caracteres, que podrían ser strings concatenados, identificadores únicos o errores de tokenización. Para reducir aún más el ruido, también se eliminaron frases genéricas frecuentes como \"assist\", \"please\", \"help\", \"thanks\", entre otras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "011cd541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palabras únicas después de limpiar: 2801\n",
      "Número de palabras con más de 25 caracteres después de limpiar: 0\n"
     ]
    }
   ],
   "source": [
    "# Filtrar cada texto eliminando palabras con frecuencia 1 o demasiado largas\n",
    "def limpiar_palabras_residuales(text):\n",
    "    return \" \".join([\n",
    "        word for word in text.split()\n",
    "        if word_freq[word] > 1 and len(word) <= 25\n",
    "    ])\n",
    "\n",
    "# Aplicar la limpieza\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].apply(limpiar_palabras_residuales)\n",
    "\n",
    "all_words_cleaned = \" \".join(data_limpia[\"Ticket Description\"]).split()\n",
    "freq_cleaned = Counter(all_words_cleaned)\n",
    "\n",
    "# Estadísticas finales\n",
    "unicas_restantes = len(freq_cleaned)\n",
    "largas_restantes = sum(1 for word in freq_cleaned if len(word) > 25)\n",
    "\n",
    "print(f\"Número de palabras únicas después de limpiar: {unicas_restantes}\")\n",
    "print(f\"Número de palabras con más de 25 caracteres después de limpiar: {largas_restantes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf2a82",
   "metadata": {},
   "source": [
    "Reducir la cantidad de palabras únicas era necesario para eliminar ruido, mejorar la calidad del análisis y optimizar el rendimiento del modelo. Depende del objetivo y del tamaño del corpus, pero en general sí conviene eliminar palabras con frecuencia 1 si representan una proporción significativa (como en este caso: 3720 de más de 5000). Estas palabras suelen ser errores tipográficos, términos irrelevantes o muy específicos, y en modelos basados en bolsa de palabras o TF-IDF aportan muy poco valor o solo introducen ruido. Al limpiar el vocabulario, se obtiene una representación más consistente y útil del texto, lo que facilita el aprendizaje del modelo y reduce la complejidad computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6b11745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  \n",
    "\n",
    "def lemmatize(text):\n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens, lang='eng')\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "68f534a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir listas de palabras clave con diferente peso semántico\n",
    "urgency_keywords_strong = [\n",
    "    'urgent', 'immediately', 'asap', 'critical', 'emergency',\n",
    "    'crash', 'system down', 'not working', 'broken', 'malfunction',\n",
    "    'lose data', 'lost data', 'data loss', 'blocked', 'unresponsive',\n",
    "    'no solution', 'bug', 'glitch', 'failure', 'can’t use', 'not responding',\n",
    "    'not function', 'battery drain', 'sudden drop', 'hardware problem'\n",
    "]\n",
    "\n",
    "urgency_keywords_weak = [\n",
    "    'problem', 'issue', 'error', 'didnt work', 'try', 'unable',\n",
    "    'update', 'reset', 'settings', 'not working properly', 'slow',\n",
    "    'not stable', 'intermittent', 'trouble', 'step', 'contact support',\n",
    "    'strange', 'noise', 'performance', 'use', 'device', 'software',\n",
    "    'notice', 'change', 'since update', 'after update'\n",
    "]\n",
    "\n",
    "# Función de detección de urgencia con puntuación ponderada\n",
    "def detect_urgencia_score(text):\n",
    "    text = str(text).lower()\n",
    "    score = 0\n",
    "\n",
    "    # Palabras fuertes (suman 2 puntos)\n",
    "    for keyword in urgency_keywords_strong:\n",
    "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "            score += 2\n",
    "\n",
    "    # Palabras débiles (suman 1 punto)\n",
    "    for keyword in urgency_keywords_weak:\n",
    "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "            score += 1\n",
    "\n",
    "    # Umbral dinámico ajustado desde el texto\n",
    "    return 1 if score >= 6 else 0  # Ajusta este umbral según tus resultados\n",
    "\n",
    "# Manejo de valores nulos\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].fillna(\"sin descripcion\")\n",
    "\n",
    "# Aplicar función al dataset\n",
    "data_limpia['Urgency_from_Text'] = data_limpia['Ticket Description'].apply(detect_urgencia_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9b28c4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tickets: 8469\n",
      "Tickets donde Urgency y Urgency_from_Text coinciden: 4301\n",
      "Porcentaje de coincidencia: 50.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_23724\\1814054218.py:17: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  no_coinciden.to_csv('C:\\CC219-TP-TF-2024-2--CC92\\data/tickets_no_coinciden.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "# Identificar casos donde Urgency y Urgency_from_Text no coinciden\n",
    "data_limpia['Coincide'] = (data_limpia['Urgency'] == data_limpia['Urgency_from_Text'])\n",
    "\n",
    "# Filtrar tickets donde haya desacuerdo\n",
    "no_coinciden = data_limpia[~data_limpia['Coincide']]\n",
    "\n",
    "# Contar cuántos casos coinciden entre Urgency y Urgency_from_Text\n",
    "coincidencias = (data_limpia['Urgency'] == data_limpia['Urgency_from_Text']).sum()\n",
    "total_tickets = len(data_limpia)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(f\"Total de tickets: {total_tickets}\")\n",
    "print(f\"Tickets donde Urgency y Urgency_from_Text coinciden: {coincidencias}\")\n",
    "print(f\"Porcentaje de coincidencia: {(coincidencias / total_tickets) * 100:.2f}%\")\n",
    "\n",
    "# Guardar estos casos para revisión\n",
    "no_coinciden.to_csv('C:\\CC219-TP-TF-2024-2--CC92\\data/tickets_no_coinciden.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6d11b",
   "metadata": {},
   "source": [
    "Tras analizar el archivo, pudimos extraer conclusiones como:\n",
    "- Muchos tickets marcados como \"Critical\" o \"High\" no tienen lenguaje urgente.\n",
    "- Algunos tickets con frases como “system down”, “lost data”, “not working” están siendo ignorados por tener prioridad baja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "47d3da55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Caso A: Tickets marcados como urgentes (manual) pero NO identificados como tales desde el texto\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------+\n",
      "| Ticket Description                                                                                                                                                                             | Ticket Priority   | Ticket Channel   |   Urgency |   Urgency_from_Text |\n",
      "|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------|\n",
      "| bill code appreciate request website address double check email address try troubleshooting step mention user manual persists                                                                  | Critical          | Social media     |         1 |                   0 |\n",
      "| need change exist product face intermittent sometimes work fine time act unexpectedly                                                                                                          | Critical          | Chat             |         1 |                   0 |\n",
      "| unable access account keep display invalid credential error even though use correct login information regain access account solution unable find option perform desire action could guide step | Critical          | Social media     |         1 |                   0 |\n",
      "| contact supplier confirm try find whether inventory currently stock reason perform factory reset hop would resolve problem didnt                                                               | Critical          | Social media     |         1 |                   0 |\n",
      "| make strange noise function properly suspect might hardware send request face intermittent sometimes work fine time act unexpectedly                                                           | Critical          | Phone            |         1 |                   0 |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------+\n",
      "\n",
      "📌 Caso B: Tickets con urgencia detectada en texto pero NO priorizados manualmente\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------+\n",
      "| Ticket Description                                                                                                                                                                             | Ticket Priority   | Ticket Channel   |   Urgency |   Urgency_from_Text |\n",
      "|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------|\n",
      "| encounter software bug whenever try perform specific action application crash update fix available window vista possible performed factory reset hop would resolve problem didnt               | Low               | Social media     |         0 |                   1 |\n",
      "| notice software bug app cause data loss unexpected error resolve need ance soon possible affect work productivity                                                                              | Medium            | Chat             |         0 |                   1 |\n",
      "| encounter software bug whenever try perform specific action application crash update fix available several package run memory default experience multiple device model seem widespread problem | Medium            | Chat             |         0 |                   1 |\n",
      "| notice software bug app cause data loss unexpected error resolve l use could guide step                                                                                                        | Low               | Chat             |         0 |                   1 |\n",
      "| seem glitch software freeze frequently make difficult use provide solution love notice peculiar error message pop screen say mean                                                              | Medium            | Phone            |         0 |                   1 |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------+\n",
      "\n",
      "Palabras más comunes en tickets urgentes no detectados:\n",
      "[('software', 313), ('update', 251), ('problem', 242), ('data', 199), ('bug', 180), ('use', 169), ('try', 149), ('device', 148), ('make', 139), ('crash', 134), ('perform', 126), ('specific', 123), ('resolve', 121), ('fix', 120), ('available', 119), ('action', 108), ('seem', 108), ('application', 106), ('error', 105), ('notice', 102)]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "data_no_coinciden = pd.read_csv('C:/CC219-TP-TF-2024-2--CC92/data/tickets_no_coinciden.csv')\n",
    "\n",
    "# Separar los tickets según tipo de discrepancia\n",
    "caso_A = data_no_coinciden[data_no_coinciden['Urgency'] == 1]   # Urgente manual, no detectado en texto\n",
    "caso_B = data_no_coinciden[data_no_coinciden['Urgency_from_Text'] == 1]  # Urgente en texto, no marcado manualmente\n",
    "\n",
    "# Seleccionar solo las columnas relevantes para mostrar\n",
    "columns_to_show = [\"Ticket Description\", \"Ticket Priority\", \"Ticket Channel\", \"Urgency\", \"Urgency_from_Text\"]\n",
    "\n",
    "print(\"\\n📌 Caso A: Tickets marcados como urgentes (manual) pero NO identificados como tales desde el texto\")\n",
    "print(tabulate(caso_A[columns_to_show].head(5), headers=\"keys\", tablefmt=\"psql\", showindex=False))\n",
    "\n",
    "print(\"\\n📌 Caso B: Tickets con urgencia detectada en texto pero NO priorizados manualmente\")\n",
    "print(tabulate(caso_B[columns_to_show].head(5), headers=\"keys\", tablefmt=\"psql\", showindex=False))\n",
    "\n",
    "# Extraer palabras de tickets urgentes no identificados manualmente\n",
    "urgentes_no_detectados = caso_B['Ticket Description'].str.lower().str.cat(sep=' ')\n",
    "palabras = re.findall(r'\\b\\w{3,}\\b', urgentes_no_detectados)\n",
    "\n",
    "# Contar palabras más frecuentes\n",
    "word_counts = Counter(palabras).most_common(20)\n",
    "print(\"\\nPalabras más comunes en tickets urgentes no detectados:\")\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7b03f",
   "metadata": {},
   "source": [
    "**Caso A: Urgencia marcada manualmente pero NO detectada desde el texto**\n",
    "\n",
    "Estos tickets fueron clasificados como urgentes por criterio humano (prioridad \"Critical\" o \"High\"), pero no contienen señales claras de urgencia en su descripción textual .\n",
    "Es decir:\n",
    "- El cliente no usó términos como \"urgent\", \"immediately\", \"system down\", etc.\n",
    "- La urgencia fue determinada sin un análisis claro del problema.\n",
    "- Esto refuerza el Insight 1 : “La gestión manual de prioridades no garantiza atención prioritaria a casos realmente críticos.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b637e26",
   "metadata": {},
   "source": [
    "**Caso B: Urgencia detectada en el texto pero NO priorizada manualmente**\n",
    "\n",
    "Estos tickets contienen lenguaje claramente urgente (\"problem\", \"doesnt respond\", \"factory reset\", \"remains unresolved\") pero fueron etiquetados como no urgentes (Low o Medium) por los operadores humanos .\n",
    "Esto confirma el Insight 1 y respalda aún más la necesidad de un sistema automatizado:\n",
    "- Los agentes no siempre logran identificar la gravedad real de los problemas.\n",
    "- Algunos clientes expresan con claridad situaciones críticas que afectan su experiencia, pero no reciben respuesta inmediata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "225cd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la columna Urgency_Final basada en un OR lógico entre ambas fuentes\n",
    "data_limpia['Urgency_Final'] = data_limpia.apply(\n",
    "    lambda row: 1 if (row['Urgency'] == 1 or row['Urgency_from_Text'] == 1) else 0,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738084e",
   "metadata": {},
   "source": [
    "La columna Urgency_Final representa una clasificación combinada de urgencia.\n",
    "Se marca como urgente (1) si al menos una de las dos fuentes lo considera urgente:\n",
    "\n",
    "- Urgency: etiqueta apartir de la clasificación del dataset, se colocó \"Urgente\" (1) a los que estan clasificados como \"Critial\" o \"Hight\" y \"No Urgente\" (0), \"Low\" o \"Medium\". \n",
    "- Urgency_from_Text: etiqueta automática basada en el contenido del texto del ticket.\n",
    "\n",
    "Si ambas lo consideran no urgente (0), entonces se marca como no urgente (0).\n",
    "\n",
    "Este enfoque busca no pasar por alto ningún posible caso urgente, combinando la evaluación humana con la automatizada mediante una lógica de tipo \"OR\" lógico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8f665c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_23724\\1558628782.py:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  data_limpia.to_csv('C:\\CC219-TP-TF-2024-2--CC92\\data/data_limpia2.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "data_limpia.to_csv('C:\\CC219-TP-TF-2024-2--CC92\\data/data_limpia2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
