{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3d22cf",
   "metadata": {},
   "source": [
    "# Parte 3: Limpieza del texto para modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8261277",
   "metadata": {},
   "source": [
    "El preprocesamiento del texto es esencial para reducir el ruido y mejorar la calidad del aprendizaje autom√°tico. El objetivo de esta etapa es:\n",
    "\n",
    "- Eliminar palabras gen√©ricas o irrelevantes\n",
    "- Reducir vocabulario redundante (palabras con frecuencia 1)\n",
    "- Filtrar tokens an√≥malos (muy largos o poco informativos)\n",
    "- Optimizar el corpus para enfoques como TF-IDF y embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56c6dd",
   "metadata": {},
   "source": [
    "### Importar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1dfc2c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tabulate in c:\\users\\alexa\\appdata\\roaming\\python\\python312\\site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "178a8cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d75d2b",
   "metadata": {},
   "source": [
    "### Carga y Limpieza del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5b59291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_23724\\1043033834.py:2: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  data_limpia = pd.read_csv('C:\\CC219-TP-TF-2024-2--CC92\\data\\data_limpia.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (8469, 11)\n",
      "\n",
      "Primeras filas del dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer Age</th>\n",
       "      <th>Customer Gender</th>\n",
       "      <th>Product Purchased</th>\n",
       "      <th>Date of Purchase</th>\n",
       "      <th>Ticket Type</th>\n",
       "      <th>Ticket Subject</th>\n",
       "      <th>Ticket Description</th>\n",
       "      <th>Ticket Status</th>\n",
       "      <th>Ticket Priority</th>\n",
       "      <th>Ticket Channel</th>\n",
       "      <th>Hours_to_First_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>Other</td>\n",
       "      <td>GoPro Hero</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>Product setup</td>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Pending Customer Response</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Social media</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>LG Smart TV</td>\n",
       "      <td>2021-05-22</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>Peripheral compatibility</td>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Pending Customer Response</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>Other</td>\n",
       "      <td>Dell XPS</td>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>Technical issue</td>\n",
       "      <td>Network problem</td>\n",
       "      <td>I'm facing a problem with my {product_purchase...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social media</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>Female</td>\n",
       "      <td>Microsoft Office</td>\n",
       "      <td>2020-11-13</td>\n",
       "      <td>Billing inquiry</td>\n",
       "      <td>Account access</td>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social media</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>Female</td>\n",
       "      <td>Autodesk AutoCAD</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>Billing inquiry</td>\n",
       "      <td>Data loss</td>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Low</td>\n",
       "      <td>Email</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer Age Customer Gender Product Purchased Date of Purchase  \\\n",
       "0            32           Other        GoPro Hero       2021-03-22   \n",
       "1            42          Female       LG Smart TV       2021-05-22   \n",
       "2            48           Other          Dell XPS       2020-07-14   \n",
       "3            27          Female  Microsoft Office       2020-11-13   \n",
       "4            67          Female  Autodesk AutoCAD       2020-02-04   \n",
       "\n",
       "       Ticket Type            Ticket Subject  \\\n",
       "0  Technical issue             Product setup   \n",
       "1  Technical issue  Peripheral compatibility   \n",
       "2  Technical issue           Network problem   \n",
       "3  Billing inquiry            Account access   \n",
       "4  Billing inquiry                 Data loss   \n",
       "\n",
       "                                  Ticket Description  \\\n",
       "0  I'm having an issue with the {product_purchase...   \n",
       "1  I'm having an issue with the {product_purchase...   \n",
       "2  I'm facing a problem with my {product_purchase...   \n",
       "3  I'm having an issue with the {product_purchase...   \n",
       "4  I'm having an issue with the {product_purchase...   \n",
       "\n",
       "               Ticket Status Ticket Priority Ticket Channel  \\\n",
       "0  Pending Customer Response        Critical   Social media   \n",
       "1  Pending Customer Response        Critical           Chat   \n",
       "2                     Closed             Low   Social media   \n",
       "3                     Closed             Low   Social media   \n",
       "4                     Closed             Low          Email   \n",
       "\n",
       "   Hours_to_First_Response  \n",
       "0                      7.0  \n",
       "1                      6.0  \n",
       "2                      7.0  \n",
       "3                      6.0  \n",
       "4                     20.0  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset limpio\n",
    "data_limpia = pd.read_csv('C:\\CC219-TP-TF-2024-2--CC92\\data\\data_limpia.csv')\n",
    "\n",
    "# Revisar el dataset\n",
    "print(\"Dimensiones del dataset:\", data_limpia.shape)\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "data_limpia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e5848512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables objetivo\n",
    "# Se consideran \"urgentes\" los tickets con prioridad \"Critical\" o \"High\" (valor 1), el resto son \"no urgentes\" (valor 0)\n",
    "data_limpia[\"Urgency\"] = data_limpia[\"Ticket Priority\"].apply(lambda x: 1 if x in [\"Critical\", \"High\"] else 0)\n",
    "\n",
    "# Tickets con tiempo de respuesta igual o menor a 1 hora toman el valor de 1, el resto como 0\n",
    "data_limpia[\"Resolution_Time_Bin\"] = data_limpia[\"Hours_to_First_Response\"].apply(lambda x: 1 if x <= 1 else 0)\n",
    "\n",
    "# Filtrar columnas relevantes\n",
    "data_limpia = data_limpia[[\"Ticket Description\", \"Ticket Priority\", \"Ticket Channel\", \"Urgency\", \"Resolution_Time_Bin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a2f1efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urgencia:\n",
      "- No urgente (0): 4255 (50.24%)\n",
      "- Urgente (1): 4214 (49.76%)\n",
      "\n",
      "Tiempo de resoluci√≥n:\n",
      "- M√°s de una hora (0): 8140 (96.12%)\n",
      "- Menos de una hora (1): 329 (3.88%)\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n\n",
    "total = len(data_limpia)\n",
    "\n",
    "# Urgencia\n",
    "urg_counts = data_limpia[\"Urgency\"].value_counts()\n",
    "print(f\"Urgencia:\")\n",
    "print(f\"- No urgente (0): {urg_counts[0]} ({urg_counts[0] / total:.2%})\")\n",
    "print(f\"- Urgente (1): {urg_counts[1]} ({urg_counts[1] / total:.2%})\\n\")\n",
    "\n",
    "# Tiempo de resoluci√≥n\n",
    "res_counts = data_limpia[\"Resolution_Time_Bin\"].value_counts()\n",
    "print(f\"Tiempo de resoluci√≥n:\")\n",
    "print(f\"- M√°s de una hora (0): {res_counts[0]} ({res_counts[0] / total:.2%})\")\n",
    "print(f\"- Menos de una hora (1): {res_counts[1]} ({res_counts[1] / total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "78db2c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticket Description</th>\n",
       "      <th>Ticket Priority</th>\n",
       "      <th>Ticket Channel</th>\n",
       "      <th>Urgency</th>\n",
       "      <th>Resolution_Time_Bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Social media</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Chat</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm facing a problem with my {product_purchase...</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social media</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Low</td>\n",
       "      <td>Social media</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm having an issue with the {product_purchase...</td>\n",
       "      <td>Low</td>\n",
       "      <td>Email</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Ticket Description Ticket Priority  \\\n",
       "0  I'm having an issue with the {product_purchase...        Critical   \n",
       "1  I'm having an issue with the {product_purchase...        Critical   \n",
       "2  I'm facing a problem with my {product_purchase...             Low   \n",
       "3  I'm having an issue with the {product_purchase...             Low   \n",
       "4  I'm having an issue with the {product_purchase...             Low   \n",
       "\n",
       "  Ticket Channel  Urgency  Resolution_Time_Bin  \n",
       "0   Social media        1                    0  \n",
       "1           Chat        1                    0  \n",
       "2   Social media        0                    0  \n",
       "3   Social media        0                    0  \n",
       "4          Email        0                    0  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_limpia.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93819ba",
   "metadata": {},
   "source": [
    "Se crean dos variables clave para entrenar los modelos de clasificaci√≥n:\n",
    "- Urgency: marca los tickets como urgentes (1) si su prioridad es Critical o High, y como no urgentes (0) en otros casos.\n",
    "- Resolution_Time_Bin: indica si el ticket recibi√≥ una primera respuesta en menos de una hora (1) o no (0).\n",
    "\n",
    "Luego, se filtran las columnas m√°s relevantes: la descripci√≥n del ticket, el canal por el que ingres√≥ y las dos nuevas etiquetas objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5c820",
   "metadata": {},
   "source": [
    "Estas variables son relevantes porque se relacionan directamente con los insights obtenidos:\n",
    "\n",
    "* **Urgency** permite identificar si un ticket es cr√≠tico o no seg√∫n su prioridad. Esto es √∫til para entrenar modelos que ayuden a mejorar la **priorizaci√≥n autom√°tica de tickets urgentes** (Insight 1).\n",
    "\n",
    "* **Resolution\\_Time\\_Bin** indica si la primera respuesta fue r√°pida (menos de una hora). Al relacionarla con el canal de entrada, permite evaluar el **desempe√±o de los canales** y encontrar oportunidades de mejora (Insight 2).\n",
    "\n",
    "* **Ticket Description** proporciona el texto del problema, que puede contener patrones comunes relacionados con urgencia o demoras, ayudando a **detectar temas frecuentes y optimizar la respuesta** (Insight 3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e7b77",
   "metadata": {},
   "source": [
    "### Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4c264a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Limpieza de texto\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\{.*?\\}', '', text)                    # Eliminar placeholders como {product_purchased}\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)                # Eliminar todo excepto letras y espacios\n",
    "    text = text.lower()                                    # Pasar todo a min√∫sculas\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)                # Eliminar palabras de 1 o 2 letras (como \"i\", \"as\", \"is\")\n",
    "    text = re.sub(r'\\s+', ' ', text)                       # Reemplazar m√∫ltiples espacios por uno solo\n",
    "    words = text.split()                                   # Divide el texto en palabras individuales\n",
    "    words = [w for w in words if w not in stop_words]      # Elimina stopwords\n",
    "    return ' '.join(words)                                 # Reconstruye el texto limpio como una sola cadena\n",
    "\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].apply(clean_text)\n",
    "\n",
    "# Codificaci√≥n de variables categ√≥ricas\n",
    "categorical_features = [\"Ticket Priority\", \"Ticket Channel\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Pipeline para vectorizar el texto con TF-IDF (limita a las 5000 palabras m√°s relevantes y elimina stopwords adicionales)\n",
    "text_transformer = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english'))])\n",
    "\n",
    "# Combinaci√≥n de transformadores: texto (TF-IDF) + variables categ√≥ricas (one-hot)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', text_transformer, \"Ticket Description\"),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1050bc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del conjunto transformado: (8469, 5008)\n",
      "ive encountered data loss issue files documents seem disappeared guide retrieve meantime ill continue develop new products thank ive checked available software updates none\n",
      "\n",
      "Categor√≠as codificadas: ['Ticket Priority_Critical' 'Ticket Priority_High' 'Ticket Priority_Low']\n",
      "Palabras TF-IDF m√°s comunes: ['ability' 'able' 'abovementioned' 'abovethen' 'abroad']\n"
     ]
    }
   ],
   "source": [
    "# Aplicar el preprocesamiento a una muestra\n",
    "X_preprocessed = preprocessor.fit_transform(data_limpia)\n",
    "\n",
    "# Imprimir forma del resultado\n",
    "print(\"Forma del conjunto transformado:\", X_preprocessed.shape)\n",
    "\n",
    "# Verificar limpieza de texto\n",
    "print(data_limpia[\"Ticket Description\"].sample(1).values[0])\n",
    "\n",
    "# Verificar categor√≠as detectadas\n",
    "ohe = preprocessor.named_transformers_['cat']['onehot']\n",
    "print(\"\\nCategor√≠as codificadas:\", ohe.get_feature_names_out(categorical_features)[:3])\n",
    "\n",
    "# Verificar palabras clave extra√≠das del texto\n",
    "tfidf = preprocessor.named_transformers_['tfidf']['tfidf']\n",
    "print(\"Palabras TF-IDF m√°s comunes:\", tfidf.get_feature_names_out()[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a6ce7",
   "metadata": {},
   "source": [
    "### Interpretaci√≥n de la salida transformada\n",
    "\n",
    "- **Forma del conjunto transformado: (8469, 5008)**  \n",
    "  Esto indica que, despu√©s de aplicar la transformaci√≥n de texto y la codificaci√≥n de variables categ√≥ricas, contamos con un total de 8469 ejemplos (tickets) y 5008 caracter√≠sticas distintas por ejemplo. Estas caracter√≠sticas incluyen tanto las palabras relevantes del texto como las variables categ√≥ricas transformadas (como prioridad o canal del ticket).\n",
    "\n",
    "- **Ejemplo de texto limpio:**  \n",
    "  Se muestra un ejemplo del contenido de un ticket despu√©s de la limpieza. En esta etapa, se eliminaron caracteres especiales, palabras vac√≠as (stopwords), y palabras poco informativas, dejando solo los t√©rminos relevantes para el modelo. Esto mejora la calidad de la representaci√≥n del texto.\n",
    "\n",
    "- **Categor√≠as codificadas:**  \n",
    "  Se listan las variables categ√≥ricas transformadas mediante codificaci√≥n one-hot. En este caso, corresponden a los diferentes niveles de prioridad del ticket. Cada una se convierte en una columna binaria que indica su presencia.\n",
    "\n",
    "- **Palabras TF-IDF m√°s comunes:**  \n",
    "  Aqu√≠ se muestran algunas de las palabras con mayor peso o frecuencia relativa dentro del corpus, despu√©s de aplicar la vectorizaci√≥n TF-IDF. Estas palabras representan los t√©rminos m√°s relevantes para el conjunto de tickets seg√∫n su frecuencia y distribuci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "dfb8834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras m√°s comunes seg√∫n TF-IDF: ['issue' 'assist' 'ive' 'problem' 'product' 'software' 'steps' 'data'\n",
      " 'persists' 'noticed' 'resolve' 'update' 'device' 'help' 'unable' 'tried'\n",
      " 'started' 'need' 'times' 'using']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = preprocessor.named_transformers_['tfidf'].named_steps['tfidf']\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf_scores = vectorizer.idf_\n",
    "\n",
    "# Mostrar top 20 palabras con menor IDF (m√°s frecuentes)\n",
    "top_indices = np.argsort(idf_scores)[:20]\n",
    "top_words = feature_names[top_indices]\n",
    "\n",
    "print(\"Palabras m√°s comunes seg√∫n TF-IDF:\", top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2301e28",
   "metadata": {},
   "source": [
    "Imprimir las 20 palabras m√°s comunes nos permite tener una visi√≥n general de qu√© t√©rminos est√°n teniendo mayor peso dentro del corpus de texto. Esta revisi√≥n es √∫til para validar si el proceso de limpieza textual fue efectivo, as√≠ como para identificar si las palabras que destacan son realmente relevantes para el an√°lisis. Tambi√©n puede ayudarnos a detectar ruido, redundancias o t√©rminos poco informativos que ser√≠a conveniente filtrar antes de entrenar un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a742f",
   "metadata": {},
   "source": [
    "Estas palabras reflejan un patr√≥n: \n",
    "- Los clientes reportan problemas t√©cnicos (bugs, errores en software/dispositivos).\n",
    "- Muchos buscan ayuda inmediata (‚Äúhelp‚Äù, ‚Äúresolve‚Äù, ‚Äúunable‚Äù, ‚Äúneed‚Äù).\n",
    "- Algunos describen acciones previas intentadas (‚Äútried‚Äù, ‚Äústeps‚Äù, ‚Äústarted‚Äù).\n",
    "\n",
    "Este hallazgo respalda el Insight 3 del EDA: la ausencia de un problema dominante refleja m√∫ltiples frentes de mejora operativa.\n",
    "\n",
    "La diversidad de palabras recurrentes sugiere que los clientes enfrentan una amplia gama de inconvenientes, no un √∫nico problema repetido. Por tanto, una soluci√≥n automatizada debe aprender a priorizar casos bas√°ndose en estas palabras clave y su contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9f1c3d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1825    issue please assist thanks return thanks under...\n",
       "473     issue please assist protected stringproperty n...\n",
       "3509    encountering software bug whenever try perform...\n",
       "5415    issue please assist please help order fulfillm...\n",
       "5537    issue please assist gnome mmt addressing servi...\n",
       "Name: Ticket Description, dtype: object"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_limpia[\"Ticket Description\"].sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a7adfc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1825        return  understanding  sharing   forget   ...\n",
       "473        protected stringproperty name productpurcha...\n",
       "3509    encountering software bug whenever try perform...\n",
       "5415         order fulfillment process  patience patie...\n",
       "5537       gnome mmt addressing service found device s...\n",
       "Name: Ticket Description, dtype: object"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_phrases = ['assist', 'please', 'help', 'issue', 'thanks', 'thank', 'sorry', 'ive' ]\n",
    "def remove_generic_phrases(text):\n",
    "    for phrase in generic_phrases:\n",
    "        text = text.replace(phrase, '')\n",
    "    return text\n",
    "\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].apply(remove_generic_phrases)\n",
    "data_limpia[\"Ticket Description\"].sample(5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "fdf565da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de palabras con frecuencia 1: 3716\n",
      "N√∫mero de palabras con m√°s de 25 caracteres: 161\n",
      "N√∫mero de palabras con caracteres no alfab√©ticos: 0\n"
     ]
    }
   ],
   "source": [
    "todas_las_palabras = ' '.join(data_limpia[\"Ticket Description\"]).split()\n",
    "\n",
    "# Contar frecuencia de cada palabra\n",
    "word_freq = Counter(todas_las_palabras)\n",
    "\n",
    "# 1. Palabras con frecuencia 1\n",
    "palabras_frec_1 = [word for word, freq in word_freq.items() if freq == 1]\n",
    "\n",
    "# 2. Palabras con m√°s de 25 caracteres\n",
    "palabras_largas = [word for word in word_freq if len(word) > 25]\n",
    "\n",
    "# 3. Palabras que contienen algo no alfab√©tico\n",
    "palabras_no_alfabeticas = [word for word in word_freq if not word.isalpha()]\n",
    "\n",
    "# Imprimir resumen\n",
    "print(f\"N√∫mero de palabras con frecuencia 1: {len(palabras_frec_1)}\")\n",
    "print(f\"N√∫mero de palabras con m√°s de 25 caracteres: {len(palabras_largas)}\")\n",
    "print(f\"N√∫mero de palabras con caracteres no alfab√©ticos: {len(palabras_no_alfabeticas)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e79cd",
   "metadata": {},
   "source": [
    "Aunque se realiz√≥ una limpieza b√°sica del texto (eliminando caracteres no alfab√©ticos, palabras cortas y stopwords), a√∫n persisten ciertos indicadores de \"ruido\" residual en el corpus. Por ejemplo, se detectaron 3720 palabras con frecuencia 1, lo que puede incluir errores tipogr√°ficos, nombres propios poco comunes o tokens irrelevantes. Adem√°s, se identificaron 162 palabras con m√°s de 25 caracteres, que podr√≠an ser strings concatenados, identificadores √∫nicos o errores de tokenizaci√≥n. Para reducir a√∫n m√°s el ruido, tambi√©n se eliminaron frases gen√©ricas frecuentes como \"assist\", \"please\", \"help\", \"thanks\", entre otras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "011cd541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de palabras √∫nicas despu√©s de limpiar: 2801\n",
      "N√∫mero de palabras con m√°s de 25 caracteres despu√©s de limpiar: 0\n"
     ]
    }
   ],
   "source": [
    "# Filtrar cada texto eliminando palabras con frecuencia 1 o demasiado largas\n",
    "def limpiar_palabras_residuales(text):\n",
    "    return \" \".join([\n",
    "        word for word in text.split()\n",
    "        if word_freq[word] > 1 and len(word) <= 25\n",
    "    ])\n",
    "\n",
    "# Aplicar la limpieza\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].apply(limpiar_palabras_residuales)\n",
    "\n",
    "all_words_cleaned = \" \".join(data_limpia[\"Ticket Description\"]).split()\n",
    "freq_cleaned = Counter(all_words_cleaned)\n",
    "\n",
    "# Estad√≠sticas finales\n",
    "unicas_restantes = len(freq_cleaned)\n",
    "largas_restantes = sum(1 for word in freq_cleaned if len(word) > 25)\n",
    "\n",
    "print(f\"N√∫mero de palabras √∫nicas despu√©s de limpiar: {unicas_restantes}\")\n",
    "print(f\"N√∫mero de palabras con m√°s de 25 caracteres despu√©s de limpiar: {largas_restantes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf2a82",
   "metadata": {},
   "source": [
    "Reducir la cantidad de palabras √∫nicas era necesario para eliminar ruido, mejorar la calidad del an√°lisis y optimizar el rendimiento del modelo. Depende del objetivo y del tama√±o del corpus, pero en general s√≠ conviene eliminar palabras con frecuencia 1 si representan una proporci√≥n significativa (como en este caso: 3720 de m√°s de 5000). Estas palabras suelen ser errores tipogr√°ficos, t√©rminos irrelevantes o muy espec√≠ficos, y en modelos basados en bolsa de palabras o TF-IDF aportan muy poco valor o solo introducen ruido. Al limpiar el vocabulario, se obtiene una representaci√≥n m√°s consistente y √∫til del texto, lo que facilita el aprendizaje del modelo y reduce la complejidad computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6b11745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  \n",
    "\n",
    "def lemmatize(text):\n",
    "    text = str(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens, lang='eng')\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "68f534a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir listas de palabras clave con diferente peso sem√°ntico\n",
    "urgency_keywords_strong = [\n",
    "    'urgent', 'immediately', 'asap', 'critical', 'emergency',\n",
    "    'crash', 'system down', 'not working', 'broken', 'malfunction',\n",
    "    'lose data', 'lost data', 'data loss', 'blocked', 'unresponsive',\n",
    "    'no solution', 'bug', 'glitch', 'failure', 'can‚Äôt use', 'not responding',\n",
    "    'not function', 'battery drain', 'sudden drop', 'hardware problem'\n",
    "]\n",
    "\n",
    "urgency_keywords_weak = [\n",
    "    'problem', 'issue', 'error', 'didnt work', 'try', 'unable',\n",
    "    'update', 'reset', 'settings', 'not working properly', 'slow',\n",
    "    'not stable', 'intermittent', 'trouble', 'step', 'contact support',\n",
    "    'strange', 'noise', 'performance', 'use', 'device', 'software',\n",
    "    'notice', 'change', 'since update', 'after update'\n",
    "]\n",
    "\n",
    "# Funci√≥n de detecci√≥n de urgencia con puntuaci√≥n ponderada\n",
    "def detect_urgencia_score(text):\n",
    "    text = str(text).lower()\n",
    "    score = 0\n",
    "\n",
    "    # Palabras fuertes (suman 2 puntos)\n",
    "    for keyword in urgency_keywords_strong:\n",
    "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "            score += 2\n",
    "\n",
    "    # Palabras d√©biles (suman 1 punto)\n",
    "    for keyword in urgency_keywords_weak:\n",
    "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', text):\n",
    "            score += 1\n",
    "\n",
    "    # Umbral din√°mico ajustado desde el texto\n",
    "    return 1 if score >= 6 else 0  # Ajusta este umbral seg√∫n tus resultados\n",
    "\n",
    "# Manejo de valores nulos\n",
    "data_limpia[\"Ticket Description\"] = data_limpia[\"Ticket Description\"].fillna(\"sin descripcion\")\n",
    "\n",
    "# Aplicar funci√≥n al dataset\n",
    "data_limpia['Urgency_from_Text'] = data_limpia['Ticket Description'].apply(detect_urgencia_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9b28c4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tickets: 8469\n",
      "Tickets donde Urgency y Urgency_from_Text coinciden: 4301\n",
      "Porcentaje de coincidencia: 50.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_23724\\1814054218.py:17: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  no_coinciden.to_csv('C:\\CC219-TP-TF-2024-2--CC92\\data/tickets_no_coinciden.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "# Identificar casos donde Urgency y Urgency_from_Text no coinciden\n",
    "data_limpia['Coincide'] = (data_limpia['Urgency'] == data_limpia['Urgency_from_Text'])\n",
    "\n",
    "# Filtrar tickets donde haya desacuerdo\n",
    "no_coinciden = data_limpia[~data_limpia['Coincide']]\n",
    "\n",
    "# Contar cu√°ntos casos coinciden entre Urgency y Urgency_from_Text\n",
    "coincidencias = (data_limpia['Urgency'] == data_limpia['Urgency_from_Text']).sum()\n",
    "total_tickets = len(data_limpia)\n",
    "\n",
    "# Mostrar resultado\n",
    "print(f\"Total de tickets: {total_tickets}\")\n",
    "print(f\"Tickets donde Urgency y Urgency_from_Text coinciden: {coincidencias}\")\n",
    "print(f\"Porcentaje de coincidencia: {(coincidencias / total_tickets) * 100:.2f}%\")\n",
    "\n",
    "# Guardar estos casos para revisi√≥n\n",
    "no_coinciden.to_csv('C:\\CC219-TP-TF-2024-2--CC92\\data/tickets_no_coinciden.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6d11b",
   "metadata": {},
   "source": [
    "Tras analizar el archivo, pudimos extraer conclusiones como:\n",
    "- Muchos tickets marcados como \"Critical\" o \"High\" no tienen lenguaje urgente.\n",
    "- Algunos tickets con frases como ‚Äúsystem down‚Äù, ‚Äúlost data‚Äù, ‚Äúnot working‚Äù est√°n siendo ignorados por tener prioridad baja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "47d3da55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Caso A: Tickets marcados como urgentes (manual) pero NO identificados como tales desde el texto\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------+\n",
      "| Ticket Description                                                                                                                                                                             | Ticket Priority   | Ticket Channel   |   Urgency |   Urgency_from_Text |\n",
      "|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------|\n",
      "| bill code appreciate request website address double check email address try troubleshooting step mention user manual persists                                                                  | Critical          | Social media     |         1 |                   0 |\n",
      "| need change exist product face intermittent sometimes work fine time act unexpectedly                                                                                                          | Critical          | Chat             |         1 |                   0 |\n",
      "| unable access account keep display invalid credential error even though use correct login information regain access account solution unable find option perform desire action could guide step | Critical          | Social media     |         1 |                   0 |\n",
      "| contact supplier confirm try find whether inventory currently stock reason perform factory reset hop would resolve problem didnt                                                               | Critical          | Social media     |         1 |                   0 |\n",
      "| make strange noise function properly suspect might hardware send request face intermittent sometimes work fine time act unexpectedly                                                           | Critical          | Phone            |         1 |                   0 |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------+\n",
      "\n",
      "üìå Caso B: Tickets con urgencia detectada en texto pero NO priorizados manualmente\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------+\n",
      "| Ticket Description                                                                                                                                                                             | Ticket Priority   | Ticket Channel   |   Urgency |   Urgency_from_Text |\n",
      "|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------|\n",
      "| encounter software bug whenever try perform specific action application crash update fix available window vista possible performed factory reset hop would resolve problem didnt               | Low               | Social media     |         0 |                   1 |\n",
      "| notice software bug app cause data loss unexpected error resolve need ance soon possible affect work productivity                                                                              | Medium            | Chat             |         0 |                   1 |\n",
      "| encounter software bug whenever try perform specific action application crash update fix available several package run memory default experience multiple device model seem widespread problem | Medium            | Chat             |         0 |                   1 |\n",
      "| notice software bug app cause data loss unexpected error resolve l use could guide step                                                                                                        | Low               | Chat             |         0 |                   1 |\n",
      "| seem glitch software freeze frequently make difficult use provide solution love notice peculiar error message pop screen say mean                                                              | Medium            | Phone            |         0 |                   1 |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------------------+-----------+---------------------+\n",
      "\n",
      "Palabras m√°s comunes en tickets urgentes no detectados:\n",
      "[('software', 313), ('update', 251), ('problem', 242), ('data', 199), ('bug', 180), ('use', 169), ('try', 149), ('device', 148), ('make', 139), ('crash', 134), ('perform', 126), ('specific', 123), ('resolve', 121), ('fix', 120), ('available', 119), ('action', 108), ('seem', 108), ('application', 106), ('error', 105), ('notice', 102)]\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "data_no_coinciden = pd.read_csv('C:/CC219-TP-TF-2024-2--CC92/data/tickets_no_coinciden.csv')\n",
    "\n",
    "# Separar los tickets seg√∫n tipo de discrepancia\n",
    "caso_A = data_no_coinciden[data_no_coinciden['Urgency'] == 1]   # Urgente manual, no detectado en texto\n",
    "caso_B = data_no_coinciden[data_no_coinciden['Urgency_from_Text'] == 1]  # Urgente en texto, no marcado manualmente\n",
    "\n",
    "# Seleccionar solo las columnas relevantes para mostrar\n",
    "columns_to_show = [\"Ticket Description\", \"Ticket Priority\", \"Ticket Channel\", \"Urgency\", \"Urgency_from_Text\"]\n",
    "\n",
    "print(\"\\nüìå Caso A: Tickets marcados como urgentes (manual) pero NO identificados como tales desde el texto\")\n",
    "print(tabulate(caso_A[columns_to_show].head(5), headers=\"keys\", tablefmt=\"psql\", showindex=False))\n",
    "\n",
    "print(\"\\nüìå Caso B: Tickets con urgencia detectada en texto pero NO priorizados manualmente\")\n",
    "print(tabulate(caso_B[columns_to_show].head(5), headers=\"keys\", tablefmt=\"psql\", showindex=False))\n",
    "\n",
    "# Extraer palabras de tickets urgentes no identificados manualmente\n",
    "urgentes_no_detectados = caso_B['Ticket Description'].str.lower().str.cat(sep=' ')\n",
    "palabras = re.findall(r'\\b\\w{3,}\\b', urgentes_no_detectados)\n",
    "\n",
    "# Contar palabras m√°s frecuentes\n",
    "word_counts = Counter(palabras).most_common(20)\n",
    "print(\"\\nPalabras m√°s comunes en tickets urgentes no detectados:\")\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7b03f",
   "metadata": {},
   "source": [
    "**Caso A: Urgencia marcada manualmente pero NO detectada desde el texto**\n",
    "\n",
    "Estos tickets fueron clasificados como urgentes por criterio humano (prioridad \"Critical\" o \"High\"), pero no contienen se√±ales claras de urgencia en su descripci√≥n textual .\n",
    "Es decir:\n",
    "- El cliente no us√≥ t√©rminos como \"urgent\", \"immediately\", \"system down\", etc.\n",
    "- La urgencia fue determinada sin un an√°lisis claro del problema.\n",
    "- Esto refuerza el Insight 1 : ‚ÄúLa gesti√≥n manual de prioridades no garantiza atenci√≥n prioritaria a casos realmente cr√≠ticos.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b637e26",
   "metadata": {},
   "source": [
    "**Caso B: Urgencia detectada en el texto pero NO priorizada manualmente**\n",
    "\n",
    "Estos tickets contienen lenguaje claramente urgente (\"problem\", \"doesnt respond\", \"factory reset\", \"remains unresolved\") pero fueron etiquetados como no urgentes (Low o Medium) por los operadores humanos .\n",
    "Esto confirma el Insight 1 y respalda a√∫n m√°s la necesidad de un sistema automatizado:\n",
    "- Los agentes no siempre logran identificar la gravedad real de los problemas.\n",
    "- Algunos clientes expresan con claridad situaciones cr√≠ticas que afectan su experiencia, pero no reciben respuesta inmediata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "225cd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la columna Urgency_Final basada en un OR l√≥gico entre ambas fuentes\n",
    "data_limpia['Urgency_Final'] = data_limpia.apply(\n",
    "    lambda row: 1 if (row['Urgency'] == 1 or row['Urgency_from_Text'] == 1) else 0,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c738084e",
   "metadata": {},
   "source": [
    "La columna Urgency_Final representa una clasificaci√≥n combinada de urgencia.\n",
    "Se marca como urgente (1) si al menos una de las dos fuentes lo considera urgente:\n",
    "\n",
    "- Urgency: etiqueta apartir de la clasificaci√≥n del dataset, se coloc√≥ \"Urgente\" (1) a los que estan clasificados como \"Critial\" o \"Hight\" y \"No Urgente\" (0), \"Low\" o \"Medium\". \n",
    "- Urgency_from_Text: etiqueta autom√°tica basada en el contenido del texto del ticket.\n",
    "\n",
    "Si ambas lo consideran no urgente (0), entonces se marca como no urgente (0).\n",
    "\n",
    "Este enfoque busca no pasar por alto ning√∫n posible caso urgente, combinando la evaluaci√≥n humana con la automatizada mediante una l√≥gica de tipo \"OR\" l√≥gico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8f665c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp\\ipykernel_23724\\1558628782.py:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  data_limpia.to_csv('C:\\CC219-TP-TF-2024-2--CC92\\data/data_limpia2.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "data_limpia.to_csv('C:\\CC219-TP-TF-2024-2--CC92\\data/data_limpia2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
